% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizers_.R
\name{optimizer_moving_average}
\alias{optimizer_moving_average}
\title{Moving Average}
\usage{
optimizer_moving_average(
  optimizer,
  average_decay = 0.99,
  num_updates = NULL,
  start_step = 0,
  dynamic_decay = FALSE,
  name = "MovingAverage",
  ...
)
}
\arguments{
\item{optimizer}{str or tf$keras$optimizers$Optimizer that will be used to compute
and apply gradients.}

\item{average_decay}{float. Decay to use to maintain the moving averages of trained variables.}

\item{num_updates}{Optional count of the number of updates applied to variables.}

\item{start_step}{int. What step to start the moving average.}

\item{dynamic_decay}{bool. Whether to change the decay based on the number of optimizer updates. Decay will start at 0.1 and gradually increase up to average_decay after each optimizer update.}

\item{name}{Optional name for the operations created when applying gradients.
Defaults to "MovingAverage".}

\item{...}{keyword arguments. Allowed to be {clipnorm, clipvalue, lr, decay}. clipnorm is clip gradients by norm; clipvalue is clip gradients by value, decay is included for backward compatibility to allow time inverse decay of learning rate. lr is included for backward compatibility, recommended to use learning_rate instead.}
}
\value{
Optimizer for use with `keras::compile()`
}
\description{
Moving Average
}
\details{
Optimizer that computes a moving average of the variables.
Empirically it has been found that using the moving average of the
trained parameters of a deep network is better than using its trained
parameters directly. This optimizer allows you to compute this moving
average and swap the variables at save time so that any code outside
of the training loop will use by default the average values
instead of the original ones.
}
\examples{

\dontrun{

opt = tf$keras$optimizers$SGD(learning_rate)
opt = moving_average(opt)

}

}
