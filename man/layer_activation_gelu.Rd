% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_activation_gelu}
\alias{layer_activation_gelu}
\title{Gaussian Error Linear Unit}
\usage{
layer_activation_gelu(object, approximate = TRUE, ...)
}
\arguments{
\item{object}{Model or layer object}

\item{approximate}{(bool) Whether to apply approximation}

\item{...}{additional parameters to pass}
}
\value{
A tensor
}
\description{
Gaussian Error Linear Unit
}
\details{
A smoother version of ReLU generally used in the BERT or BERT architecture based
models. Original paper: https://arxiv.org/abs/1606.08415
}
\note{
Input shape: Arbitrary. Use the keyword argument `input_shape` (tuple of integers, d
oes not include the samples axis) when using this layer as the first layer in a model.

Output shape: Same shape as the input.
}
