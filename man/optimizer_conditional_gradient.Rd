% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizers.R
\name{optimizer_conditional_gradient}
\alias{optimizer_conditional_gradient}
\title{Conditional Gradient}
\usage{
optimizer_conditional_gradient(
  learning_rate,
  lambda_ = 0.01,
  epsilon = 1e-07,
  ord = "fro",
  name = "ConditionalGradient",
  ...
)
}
\arguments{
\item{learning_rate}{A Tensor or a floating point value, or a schedule that is a tf$keras$optimizers$schedules$LearningRateSchedule The learning rate.}

\item{lambda_}{A Tensor or a floating point value. The constraint.}

\item{epsilon}{A Tensor or a floating point value. A small constant for numerical stability when handling the case of norm of gradient to be zero.}

\item{ord}{Order of the norm. Supported values are 'fro' and 'nuclear'. Default is 'fro', which is frobenius norm.}

\item{name}{Optional name prefix for the operations created when applying gradients. Defaults to 'ConditionalGradient'.}

\item{...}{keyword arguments. Allowed to be {clipnorm, clipvalue, lr, decay}. clipnorm is clip gradients by norm; clipvalue is clip gradients by value, decay is included for backward compatibility to allow time inverse decay of learning rate. lr is included for backward compatibility, recommended to use learning_rate instead.}
}
\value{
Optimizer for use with `keras::compile()`
}
\description{
Conditional Gradient
}
