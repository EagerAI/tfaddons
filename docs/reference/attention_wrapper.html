<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Attention Wrapper — attention_wrapper • tfaddons</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Attention Wrapper — attention_wrapper" />
<meta property="og:description" content="Attention Wrapper" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">tfaddons</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.10.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/NMT.html">Neural Machine Translation</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/henry090/tfaddons/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Attention Wrapper</h1>
    <small class="dont-index">Source: <a href='https://github.com/henry090/tfaddons/blob/master/R/seq2seq.R'><code>R/seq2seq.R</code></a></small>
    <div class="hidden name"><code>attention_wrapper.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Attention Wrapper</p>
    </div>

    <pre class="usage"><span class='fu'>attention_wrapper</span>(
  <span class='no'>object</span>,
  <span class='no'>cell</span>,
  <span class='no'>attention_mechanism</span>,
  <span class='kw'>attention_layer_size</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>alignment_history</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>cell_input_fn</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>output_attention</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>,
  <span class='kw'>initial_cell_state</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>attention_layer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>attention_fn</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='no'>...</span>
)</pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>object</th>
      <td><p>Model or layer object</p></td>
    </tr>
    <tr>
      <th>cell</th>
      <td><p>An instance of RNNCell.</p></td>
    </tr>
    <tr>
      <th>attention_mechanism</th>
      <td><p>A list of AttentionMechanism instances or a single instance.</p></td>
    </tr>
    <tr>
      <th>attention_layer_size</th>
      <td><p>A list of Python integers or a single Python integer, the
depth of the attention (output) layer(s). If `NULL` (default), use the context as attention
at each time step. Otherwise, feed the context and cell output into the attention layer
to generate attention at each time step. If attention_mechanism is a list,
attention_layer_size must be a list of the same length. If attention_layer is set, this
must be `NULL`. If attention_fn is set, it must guaranteed that the outputs of `attention_fn`
also meet the above requirements.</p></td>
    </tr>
    <tr>
      <th>alignment_history</th>
      <td><p>Python boolean, whether to store alignment history from all time
steps in the final output state (currently stored as a time major TensorArray on which you
must call stack()).</p></td>
    </tr>
    <tr>
      <th>cell_input_fn</th>
      <td><p>(optional) A callable.
The default is: lambda inputs, attention: tf$concat(list(inputs, attention), -1).</p></td>
    </tr>
    <tr>
      <th>output_attention</th>
      <td><p>Python bool. If True (default), the output at each time step is the
attention value. This is the behavior of Luong-style attention mechanisms. If FALSE, the output
at each time step is the output of cell. This is the behavior of Bhadanau-style attention
mechanisms. In both cases, the attention tensor is propagated to the next time step via the
state and is used there. This flag only controls whether the attention mechanism is propagated
up to the next cell in an RNN stack or to the top RNN output.</p></td>
    </tr>
    <tr>
      <th>initial_cell_state</th>
      <td><p>The initial state value to use for the cell when the user calls
get_initial_state(). Note that if this value is provided now, and the user uses a batch_size
argument of get_initial_state which does not match the batch size of initial_cell_state,
proper behavior is not guaranteed.</p></td>
    </tr>
    <tr>
      <th>name</th>
      <td><p>Name to use when creating ops.</p></td>
    </tr>
    <tr>
      <th>attention_layer</th>
      <td><p>A list of tf$keras$layers$Layer instances or a single tf$keras$layers$Layer
instance taking the context and cell output as inputs to generate attention at each time step.
If `NULL` (default), use the context as attention at each time step. If attention_mechanism is a list,
attention_layer must be a list of the same length. If attention_layers_size is set, this must be `NULL`.</p></td>
    </tr>
    <tr>
      <th>attention_fn</th>
      <td><p>An optional callable function that allows users to provide their own customized
attention function, which takes input (attention_mechanism, cell_output, attention_state, attention_layer)
and outputs (attention, alignments, next_attention_state). If provided, the attention_layer_size should
be the size of the outputs of attention_fn.</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>Other keyword arguments to pass</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>None</p>
    <h2 class="hasAnchor" id="note"><a class="anchor" href="#note"></a>Note</h2>

    <p>If you are using the `decoder_beam_search` with a cell wrapped in `AttentionWrapper`, then
you must ensure that:
 - The encoder output has been tiled to `beam_width` via `tile_batch` (NOT `tf$tile`).
 - The `batch_size` argument passed to the `get_initial_state` method of this wrapper
is equal to `true_batch_size * beam_width`.
 - The initial state created with `get_initial_state` above contains a `cell_state` value
 containing properly tiled final state from the encoder.</p>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Turgut Abdullayev.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


